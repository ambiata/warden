\documentclass[a4paper]{article}

\title{warden}

\author{Sharif Olorin}

\usepackage{amssymb,amsmath}
\usepackage{hyperref}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{listings}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Purpose}\label{usage}

\texttt{warden} validates feeds of separated-value-format data (CSV,
PSV, et cetera). It is designed to be run at approximately the same
frequency as the feed's land/extract jobs. \texttt{warden check}
consists of in-depth view-level checks and is run following the
extract phase. \texttt{warden sanity} consists of rudimentary
file-level checks and can be run either before or after the extract
phase.

\section{Note regarding data security}\label{note-regarding-data-security}

The schema files generated by \texttt{warden infer} contain no data
sufficiently related to that in the input feed to infer any of the
data in the feed, and it is acceptable to store them in
\texttt{git} repositories and access them using workstations.

However, the view marker files generated by \texttt{warden check}
contain significant amounts of information entangled with specific data in
the feed (e.g., hashes of field values), and must \textit{never}
be stored on workstations or in source repositories - they should be
treated with the same care as the feed data itself.

\section{Data formats}\label{data-formats}

Warden supports two data formats, delimited text and a CSV format
based on RFC 4180.\cite{rfc4180} Both formats must be UTF-8-encoded.

\subsection{Delimited text}

This record format consists of opaque text fields delimited by a
single-character separator. Records are delimited by the newline
character (\texttt{0x0a}). This format has no escape sequences.

\subsection{RFC4180}

This record format is identical to that described in RFC 4180, with
the following exceptions:

\begin{itemize}
  \item The field separator is not required to be a comma.
  \item The line feed character \texttt{0x0a} is used as the record
    separator.
  \item The last field in the record is terminated by a record separator.
  \item The line feed character \texttt{0x0a} may not appear in
    records regardless of quoting.
\end{itemize}

In addition, the \texttt{warden} parser does not perform unescaping on
escaped fields, with the exception of removing the outer
double-quotes, as this does not affect validation (as text fields are
treated as opaque).

\section{Monitoring a new feed}\label{monitoring-a-new-feed}

\subsection{Initial checks}

To set up monitoring for a new feed, the first step is to run
\texttt{warden check} without a schema until it has accumulated
sufficient metadata to infer an accurate schema.

First, determine the feed's separator manually (or implement this
functionality in \texttt{warden}, whichever is easier). Then construct
a \texttt{warden check} command as follows:

\begin{lstlisting}[language=bash]
warden check \
  -s ${separator} \
  -r ${reservoir_size} \
  --exit-success \
  ${feed_name}
\end{lstlisting}

The \texttt{reservoir\_size} is an integer number of 64-bit words to
store for each numeric field in the feed. It should not be larger than
the expected number of new records \texttt{warden} will check at one
time, but otherwise can be as large as memory will comfortably allow;
greater values mean less error in the estimation of quantiles.

This check command should be run from a script (e.g., \texttt{o2} job
run by \texttt{hydra}) which syncs down data files which are not
marked as having been validated (\texttt{relic} mark \texttt{validate}
is \texttt{unprocessed}), runs \texttt{warden check} and syncs the
resulting markers back to \texttt{S3}. For example:

\lstinputlisting[language=bash]{vapour-validate.sh}

\subsection{Generating a schema}

Running without a schema, \texttt{warden} will only detect a few
basic classes of feed errors, such as an inconsistent field count. If
these early checks fail, you will need to contact the customer and get
them to deal with the invalid records before proceeding.

Once the check has run successfully a few times, sync down all the
view markers it has written for the relevant feed and run
\texttt{warden infer} on them:

\begin{lstlisting}[language=bash]
find _warden -type f -name \*.warden -print0 |\
    xargs -0 warden infer -o schema.json
\end{lstlisting}

This will write a file called \texttt{schema.json} in the current
directory. Inspect both it and the data to sanity-check each field;
most schemas will need some human editing before being accurate. For
example, one common case which requires human intervention is date
fields (e.g., date of a customer phone call) - given that this is an
initial run on a few days' of data, the range of dates present may be
quite limited and thus \texttt{warden} might infer that this is a
categorical field, when in fact more data would indicate that it is
freeform.

Once you have a schema, run \texttt{warden check} again with the
\texttt{--schema} option to verify that it still passes. If it does
not, you'll need to use your own judgement (and possibly communicate
with the customer) to determine whether this is an issue with the
schema or the feed.

Once the check has been running stably for a while with the schema in
place, remove the \textbf{--exit-success} option so that failures will
cause a notification to be sent.

\section{PII checks}

PII checks are currently very rudimentary, and all consist of simple
parsers.

\begin{itemize}
  \item Phone numbers - checks for numbers which are either
    Australian phone numbers with a valid state-level area code or
    look like international phone numbers (starting with \texttt{+}).
  \item Email addresses - makes no attempt to determine whether a
    string represents a valid email address per RFC 5321\cite{rfc5321}
    for performance reasons, but simply looks for a string with
    period-separated components containing an \texttt{@} symbol.
\end{itemize}

\section{Schema inference}\label{schema-inference}

Schema inference in \texttt{warden} is currently extremely simple, and will be
made more sophisticated if/when that proves necessary. It operates on
the view markers generated by \texttt{warden\ check} and the metadata
they contain.

\subsection{Data}\label{data}

The metadata is generated by \texttt{warden\ check} as follows:

\begin{itemize}
\item
  Process a view and decode it into a \texttt{Vector\ Text} of raw
  fields.
\item
  Run a \texttt{choice} parser which attempts to interpret the field as
  various different types, from most to least specific (e.g., Integer
  before Real).
\item
  Keep a tally of summed observations from each row.
\end{itemize}

\subsection{Compatibility}\label{compatibility}

Schema field types form a partially-ordered set under set inclusion
which looks kinda like a join semilattice. For example, if a given field
is classified as a \texttt{CategoricalField}, it's not an anomaly (from
the fairly limited perspective of schema inference) if some of its
values are classifed as \texttt{LooksBoolean} (e.g., from the set
\{``true'', ``false'', ``undetermined''\}).

Thus, a \texttt{FieldLooks} value \texttt{v} is said to be compatible
with a \texttt{FieldType} \texttt{t} if one would expect to see values
which look like \texttt{v} in an error-free field of type \texttt{t}.
For example, the field type \texttt{RealField} is compatible with
\texttt{LooksIntegral} observations as the integers are contained within
the reals, but not the inverse.

\begin{verbatim}
    Real+---------Text-------+ Boolean
        |
        |
        v
    Integral
\end{verbatim}

\subsection{Schema generation}\label{schema-generation}

Roughly:

\begin{itemize}
\item
  Check the markers for incompatibilities (e.g., same number of fields),
  then aggregate them.
\item
  Starting with an empty set of type-indexed counts C, for each field
  \texttt{f} and for every possible field type \texttt{t}, for each
  observation count associated with \texttt{f} which is compatible with
  \texttt{t}, increment \texttt{C\_t}.
\item
  Select as candidates all types with counts higher than a threshold
  (say 95\%). (There must be at least one of these, as Text is
  all-inclusive).
\item
  Take the minimum (most specific) type by set inclusion from the
  candidates. If there are multiple minima, revert to the most general
  \texttt{FieldType}, \texttt{Text}.\footnote{In this case, one of two
    things has happened: the field is an amalgam of two types with
    equal representation in the data, or all observations are
    \texttt{LooksEmpty}. In both of these cases, we can't infer
    anything more specific than the most general field type.}
\end{itemize}

\section{Numerics}\label{numerics}

\subsection{Numeric fields}\label{numeric-fields}

Numeric summaries are calculated for any column which has at least one
value which parses as numeric (thus, care must be taken to ensure that
only results from fields which are actually numeric are used).

\subsection{Mean calculation}\label{mean-calculation}

The mean of numeric fields is computed using the following recurrence
relation\cite[pp.~232]{Knuth1997}:

\[M_1 = x_1, M_k = M_{k-1} + \frac{x_k - M_{k-1}}{k}\]

This has the advantages of operating in a single pass and accumulating
less error on large datasets than the direct calculation
\(\mu = \frac{1}{n} \sum\limits_{i=1}^n x_i\).

A standard method in numerical computing for calculating the mean with
minimal error is to sort the dataset and then perform the sum in order;
this doesn't work for us as it requires multiple passes. An alternative
might be Kahan summation\cite{Kahan1965}, but this raises concerns about
precision lost on large datasets when storing the entire sum in one
64-bit float.

\subsection{Standard deviation and
variance}\label{standard-deviation-and-variance}

The accumulator is computed using the following recurrence relation\cite[pp.~232]{Knuth1997}, known as Welford's method\cite{welford1962}:

\[S_1 = 0, S_k = S_{k-1} + (x_k - M_{k-1})(x_k - M_k)\]

This quantity related to the variance with
\(\sigma^2 = \frac{S_n}{n-1}\).

\subsection{Combining accumulators}\label{combining-accumulators}

\texttt{warden} computes summaries of chunks of data in parallel, which results
in the need to combine accumulators of subsets of the dataset to arrive
at the final result.

\subsubsection{Mean}\label{mean}

The two terms are scaled according to the corresponding number of
observed values and then divided by the total number of observed
elements.

Letting \(\mu_{a:b}\) represent the mean of data points \(a\) through
\(b\) inclusive, and taking \(x_{1:m}\) and \(x_{m+1:n+m}\) to be the
subsets we wish to combine:

\[\mu_{1:n+m} = \frac{n(\mu_{1:n}) + m(\mu_{n:n+m})}{m + n}\]

\paragraph{Derivation}\label{derivation}

\[\mu_{1:n+m} = \frac{1}{m+n} \sum\limits_{i=1}^{m+n} x_i\]

\[\begin{aligned}(m + n) \mu_{1:n+m} &= \sum\limits_{i=1}^{n+m} x_i \\
                                     &= \sum\limits_{i=1}^n x_i + \sum\limits_{i=n}^{n+m} x_i \\
                                     &= n\mu_{1:n} + m\mu_{n:n+m}\end{aligned}\]

\[\mu_{1:n+m} = \frac{n\mu_{1:n} + m\mu_{n:n+m}}{m + n}\]

\subsubsection{Variance/standard
deviation}\label{variancestandard-deviation}

With respect to variance, many methods are available; some are decidedly
worse than others, but even the best perform badly on certain types of
data. Thus, the correct choice here depends on the distribution of the
data we expect to validate, and as \texttt{warden} is designed to work
on almost all real-world datasets there is no single optimal algorithm,
and adapting the algorithm to the dataset being examined is not
practical as the optimal algorithm depends on (among other properties of
the data distribution) its variance.\cite{Ling1974}

The accumulator is first converted to population variance. Then the two
subset variances are combined:

\[\sigma_{1:n+m}^2 = \frac{m(\sigma_{1:m}^2 + \mu_{1:m}^2) + n(\sigma_{m:n}^2 + \mu_{m:n}^2)}{m + n} - \mu_{1:n+m}^2\]

The naive floating-point computation of this value was found to be
numerically unstable and was optimised with the help of Herbie\cite{panchekha2015},
resulting in the current Haskell implementation.

Finally, the combined variance is converted back to an accumulator:

\[S_n = \sigma^2(n - 1)\]

This method remains problematic due to accumulation of floating-point
error, and requires some refinement.

\paragraph{Derivation}\label{derivation-1}

Letting \(\sigma_{a:b}^2\) represent the variance of data points \(a\)
through \(b\) inclusive, and giving \(\mu_{a:b}\) and \(x_{a:b}\) the
same meanings as for the mean, we start from the definitions of mean and
variance:

\[\mu_{1:n} = \frac{1}{n} \sum\limits_{i=1}^n x_i\]

\[\sigma_{1:n}^2 = \frac{1}{n} \sum\limits_{i=1}^n (x_i - \mu_{1:n})^2\]

We rearrange to isolate the sum-of-squares:

\[\begin{aligned}\sigma_{1:m+n}^2 &= \frac{1}{n+m} \sum\limits_{i=1}^{n+m} (x_i^2 - 2\mu_{1:n+m}x_i + \mu^2) \\
  &= \frac{\sum\limits_{i=1}^{n+m} x_i^2}{n+m} - \frac{2\mu_{1:n+m}\sum\limits_{i=1}^{n+m} x_i}{n+m} + \frac{(n + m)\mu_{1:m+n}^2}{n+m} \\
  &= \frac{\sum\limits_{i=1}^{n+m} x_i^2}{n+m} - 2\mu_{1:m+n}^2 + \mu_{1:m+n}^2 \\
  &= \frac{\sum\limits_{i=1}^{n+m} x_i^2}{n+m} - \mu_{1:m+n}^2 \\
  \sigma_{1:n+m}^2 + \mu_{1:n+m}^2 &= \frac{1}{n+m} \sum\limits_{i=1}^{n+m} x_i^2\end{aligned}\]

And express the sum in terms of the subsets \(x_{1:m}\) and
\(x_{m+1:n+m}\):

\[\begin{aligned}(m + n)(\sigma_{1:m+n}^2 + \mu_{1:m+n}^2) &= \sum\limits_{i=1}^{n+m} x_i^2 \\
  &= \sum_{i=1}^n x_i^2 + \sum_{i=n+1}^{n+m} x_i^2 \\
  &= n(\sigma^2_{1:n} + \mu_{1:n}^2) + m(\sigma_{1+n:m+n}^2 + \mu_{1+n:m+n}^2)\end{aligned}\]

Solving:

\[\sigma_{1:m+n}^2 = \frac{n(\sigma_{1:n}^2 + \mu_{1:n}^2) + m(\sigma_{1+n:m+n}^2 + \mu_{1+n:m+n}^2)}{m+n} - \mu_{1:m+n}^2\]

\subsection{Quantiles}\label{quantiles}

Quantiles are computed in one pass by creating a uniform sample of the
data using a basic reservoir
sampling\cite{vitter1985}\cite[pp.~144-145]{Knuth1997} method. A
reservoir \(R\) is maintained of the desired final sample size \(n\),
and filled as follows:

\begin{itemize}
\item
The first \(n\) records seen are added to the reservoir.

\item
For all records beyond \(n\), a value is drawn uniformly from the
interval \([1, t]\), where \(t\) is the number of records seen so
far. If \(t \leq n\), the record \(R_t\) is replaced with the current value.
\end{itemize}

Once the sample is complete, the median is calculated using the basic
sample-median algorithm: the data are sorted and the middle element is
taken if the sample size is odd, or the middle two elements are
averaged if the sample size is even.

\subsubsection{Combining accumulators}

The size of the combined acculumator \(|Z|\) is determined as
the smaller of the desired final sample size \(n\) and the sum of the
sizes of the two original samples \(X\) and \(Y\).

\(X\) and \(Y\) are appended, then randomly permuted with a
Fisher-Yates shuffle\cite[pp.~145-146]{Knuth1997}. Finally, \(|Z|\) elements are drawn from the
beginning of the combined sample to serve as the new accumulator.

\section{Anomaly detection}\label{anomaly-detection}

Epistemic status: this section is a broad outline of future
functionality. It will be refined once the methodology is finalised.

\texttt{warden} can perform numerical anomaly detection in two
different contexts: detection of anomalous values within a single
dataset, and detection of anomalies in datasets over time.

\subsection{Single-dataset anomaly detection}\label{anomaly-detection-single-dataset}

In the single-dataset case, the main input to the algorithm is a
single unordered set \( S \) of real numbers, e.g., a numeric field in
one data file. The data are assumed to represent a sample \( y \)
drawn from a random variable \( X \), which is mixed with zero or
more values \( a \) which are not drawn from \( X \). Less-formally, most of the
values are assumed to be generated by the same process (e.g., dollar
values of transactions at a supermarket) with comparatively few
anomalous values generated by some other process (e.g., data entry
errors). This functionality is best-suited to interactive use,
either internally or by clients (e.g., as a sanity check of uploaded
sample datasets).

\subsection{Detecting anomalies over time}\label{anomaly-detection-over-time}

Anomaly detection over time also involves examining a set of raw data
which purportedly represents samples from a random variable. Unlike in
the single-dataset case, here we consider the dataset in question \(
S_n \) to be part of a set of samples \( S_{1..n} \) of a random
variable parameterised by time. This gives us much more flexibility in
our approach to identifying bad data, as we can use summary statistics
generated from \( S_{1..n-1} \) to identify larger-scale anomalies
(e.g., a software bug causes today's transaction values to all be
replaced by \$3.14, an error which would be undetectable if the
dataset was viewed in isolation, at least without additional context
on semantic interpretation of the data), as well as using time-series
methods to account for seasonal variation.

\bibliographystyle{plain}
\bibliography{warden.bib}


\end{document}
