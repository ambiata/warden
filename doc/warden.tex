\documentclass[a4paper]{article}

\title{Warden}

\author{Sharif Olorin}

\usepackage{amssymb,amsmath}
\usepackage{hyperref}
\usepackage[nottoc,numbib]{tocbibind}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Schema inference}\label{schema-inference}

Schema inference in warden is currently extremely simple, and will be
made more sophisticated if/when that proves necessary. It operates on
the view markers generated by \texttt{warden\ check} and the metadata
they contain.

\subsection{Data}\label{data}

The metadata is generated by \texttt{warden\ check} as follows:

\begin{itemize}
\item
  Process a view and decode it into a \texttt{Vector\ Text} of raw
  fields.
\item
  Run a \texttt{choice} parser which attempts to interpret the field as
  various different types, from most to least specific (e.g., Integer
  before Real).
\item
  Keep a tally of summed observations from each row.
\end{itemize}

\subsection{Compatibility}\label{compatibility}

Schema field types form a partially-ordered set under set inclusion
which looks kinda like a join semilattice. For example, if a given field
is classified as a \texttt{CategoricalField}, it's not an anomaly (from
the fairly limited perspective of schema inference) if some of its
values are classifed as \texttt{LooksBoolean} (e.g., from the set
\{``true'', ``false'', ``undetermined''\}).

Thus, a \texttt{FieldLooks} value \texttt{v} is said to be compatible
with a \texttt{FieldType} \texttt{t} if one would expect to see values
which look like \texttt{v} in an error-free field of type \texttt{t}.
For example, the field type \texttt{RealField} is compatible with
\texttt{LooksIntegral} observations as the integers are contained within
the reals (sorry Liam), but not the inverse.

\begin{verbatim}
    Real+---------Text-------+ Boolean
        |
        |
        v
    Integral
\end{verbatim}

\subsection{Schema generation}\label{schema-generation}

Roughly:

\begin{itemize}
\item
  Check the markers for incompatibilities (e.g., same number of fields),
  then aggregate them.
\item
  Starting with an empty set of type-indexed counts C, for each field
  \texttt{f} and for every possible field type \texttt{t}, for each
  observation count associated with \texttt{f} which is compatible with
  \texttt{t}, increment \texttt{C\_t}.
\item
  Select as candidates all types with counts higher than a threshold
  (say 95\%). (There must be at least one of these, as Text is
  all-inclusive).
\item
  Take the minimum (most specific) type by set inclusion from the
  candidates. If there are multiple minima, fail.
\end{itemize}

\section{Numerics}\label{numerics}

\subsection{Numeric fields}\label{numeric-fields}

Numeric summaries are calculated for any column which has at least one
value which parses as numeric (thus, care must be taken to ensure that
only results from fields which are actually numeric are used).

\subsection{Mean calculation}\label{mean-calculation}

The mean of numeric fields is computed using the following recurrence
relation\cite[pp.~232]{Knuth1997}:

\[M_1 = x_1, M_k = M_{k-1} + \frac{x_k - M_{k-1}}{k}\]

This has the advantages of operating in a single pass and accumulating
less error on large datasets than the direct calculation
\(\mu = \frac{1}{n} \sum\limits_{i=1}^n x_i\).

A standard method in numerical computing for calculating the mean with
minimal error is to sort the dataset and then perform the sum in order;
this doesn't work for us as it requires multiple passes. An alternative
might be Kahan summation\cite{Kahan1965}, but this raises concerns about
precision lost on large datasets when storing the entire sum in one
64-bit float.

\subsection{Standard deviation and
variance}\label{standard-deviation-and-variance}

The accumulator is computed using the following recurrence relation\cite[pp.~232]{Knuth1997}:

\[S_1 = 0, S_k = S_{k-1} + (x_k - M_{k-1})(x_k - M_k)\]

This quantity related to the variance with
\(\sigma^2 = \frac{S_n}{n-1}\).

\subsection{Combining accumulators}\label{combining-accumulators}

Warden computes summaries of chunks of data in parallel, which results
in the need to combine accumulators of subsets of the dataset to arrive
at the final result.

\subsubsection{Mean}\label{mean}

The two terms are scaled according to the corresponding number of
observed values and then divided by the total number of observed
elements.

Letting \(\mu_{a:b}\) represent the mean of data points \(a\) through
\(b\) inclusive, and taking \(x_{1:m}\) and \(x_{m+1:n+m}\) to be the
subsets we wish to combine:

\[\mu_{1:n+m} = \frac{n(\mu_{1:n}) + m(\mu_{n:n+m})}{m + n}\]

\paragraph{Derivation}\label{derivation}

\[\mu_{1:n+m} = \frac{1}{m+n} \sum\limits_{i=1}^{m+n} x_i\]

\[\begin{aligned}(m + n) \mu_{1:n+m} &= \sum\limits_{i=1}^{n+m} x_i \\
                                     &= \sum\limits_{i=1}^n x_i + \sum\limits_{i=n}^{n+m} x_i \\
                                     &= n\mu_{1:n} + m\mu_{n:n+m}\end{aligned}\]

\[\mu_{1:n+m} = \frac{n\mu_{1:n} + m\mu_{n:n+m}}{m + n}\]

\subsubsection{Variance/standard
deviation}\label{variancestandard-deviation}

With respect to variance, many methods are available; some are decidedly
worse than others, but even the best perform badly on certain types of
data. Thus, the correct choice here depends on the distribution of the
data we expect to validate, and as \texttt{warden} is designed to work
on almost all real-world datasets there is no single optimal algorithm,
and adapting the algorithm to the dataset being examined is not
practical as the optimal algorithm depends on (among other properties of
the data distribution) its variance.\cite{Ling1974}

The accumulator is first converted to population variance. Then the two
subset variances are combined:

\[\sigma_{1:n+m}^2 = \frac{m(\sigma_{1:m}^2 + \mu_{1:m}^2) + n(\sigma_{m:n}^2 + \mu_{m:n}^2)}{m + n} - \mu_{1:n+m}^2\]

The naive floating-point computation of this value was found to be
numerically unstable and was optimised with the help of Herbie\cite{panchekha2015},
resulting in the current Haskell implementation.

Finally, the combined variance is converted back to an accumulator:

\[S_n = \sigma^2(n - 1)\]

This method remains problematic due to accumulation of floating-point
error, and requires some refinement.

\paragraph{Derivation}\label{derivation-1}

Letting \(\sigma_{a:b}^2\) represent the variance of data points \(a\)
through \(b\) inclusive, and giving \(\mu_{a:b}\) and \(x_{a:b}\) the
same meanings as for the mean, we start from the definitions of mean and
variance:

\[\mu_{1:n} = \frac{1}{n} \sum\limits_{i=1}^n x_i\]

\[\sigma_{1:n}^2 = \frac{1}{n} \sum\limits_{i=1}^n (x_i - \mu_{1:n})^2\]

We rearrange to isolate the sum-of-squares:

\[\begin{aligned}\sigma_{1:m+n}^2 &= \frac{1}{n+m} \sum\limits_{i=1}^{n+m} (x_i^2 - 2\mu_{1:n+m}x_i + \mu^2) \\
  &= \frac{\sum\limits_{i=1}^{n+m} x_i^2}{n+m} - \frac{2\mu_{1:n+m}\sum\limits_{i=1}^{n+m} x_i}{n+m} + \frac{(n + m)\mu_{1:m+n}^2}{n+m} \\
  &= \frac{\sum\limits_{i=1}^{n+m} x_i^2}{n+m} - 2\mu_{1:m+n}^2 + \mu_{1:m+n}^2 \\
  &= \frac{\sum\limits_{i=1}^{n+m} x_i^2}{n+m} - \mu_{1:m+n}^2 \\
  \sigma_{1:n+m}^2 + \mu_{1:n+m}^2 &= \frac{1}{n+m} \sum\limits_{i=1}^{n+m} x_i^2\end{aligned}\]

And express the sum in terms of the subsets \(x_{1:m}\) and
\(x_{m+1:n+m}\):

\[\begin{aligned}(m + n)(\sigma_{1:m+n}^2 + \mu_{1:m+n}^2) &= \sum\limits_{i=1}^{n+m} x_i^2 \\
  &= \sum_{i=1}^n x_i^2 + \sum_{i=n+1}^{n+m} x_i^2 \\
  &= n(\sigma^2_{1:n} + \mu_{1:n}^2) + m(\sigma_{1+n:m+n}^2 + \mu_{1+n:m+n}^2)\end{aligned}\]

Solving:

\[\sigma_{1:m+n}^2 = \frac{n(\sigma_{1:n}^2 + \mu_{1:n}^2) + m(\sigma_{1+n:m+n}^2 + \mu_{1+n:m+n}^2)}{m+n} - \mu_{1:m+n}^2\]

\subsection{Quantiles}\label{quantiles}

Quantiles are computed in one pass using a basic reservoir
sampling\cite{vitter1985}\cite[pp.~144-145]{Knuth1997} method. A reservoir \(R\) is maintained of
the desired final sample size \(n\), and filled as follows:

\begin{itemize}
\item
The first \(n\) records seen are added to the reservoir.

\item
For all records beyond \(n\), a value is drawn uniformly from the
interval \([1, t]\), where \(t\) is the number of records seen so
far. If \(t \leq n\), the record \(R_t\) is replaced with the current value.
\end{itemize}

\subsubsection{Combining accumulators}

The size of the combined acculumator \(|Z|\) is determined as
the smaller of the desired final sample size \(n\) and the sum of the
sizes of the two original samples \(X\) and \(Y\).

\(X\) and \(Y\) are appended, then randomly permuted with a
Fisher-Yates shuffle\cite[pp.~145-146]{Knuth1997}. Finally, \(|Z|\) elements are drawn from the
beginning of the combined sample to serve as the new accumulator.

\bibliographystyle{plain}
\bibliography{warden.bib}


\end{document}
